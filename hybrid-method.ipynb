{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"paper\", rc={\"lines.linewidth\": 1})\n",
    "\n",
    "rcParams['axes.titlepad'] = 20\n",
    "rcParams['axes.titlesize'] = \"medium\"\n",
    "rcParams['axes.edgecolor'] = \"red\"\n",
    "rcParams['axes.spines.right'] = rcParams['axes.spines.top'] = False\n",
    "rcParams['xtick.labelsize'] = \"small\"\n",
    "rcParams['xtick.major.pad'] = 10\n",
    "\n",
    "rcParams['ytick.labelsize'] = \"small\"\n",
    "rcParams['ytick.major.pad'] = 10\n",
    "rcParams['axes.formatter.use_mathtext'] = True\n",
    "rcParams['axes.labelpad'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/home/fat-fighter/Documents/cs771-project/hybrid-method/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Files\n",
    "\n",
    "### Folder: features\n",
    "\n",
    "- **tracks-mfcc.csv** - Contains already extracted mfcc features from all tracks using 30-60 seconds of tracks\n",
    "- **tracks-cluster-probabilities.csv** - Contains the cluster probabolities and assignments for all tracks (based on their mfcc features_\n",
    "- **timbres-cluster-probabilities.csv** - Contains the cluster probabilities and assignments for all segment timbres of all tracks\n",
    "- **tracks-collective-timbres-clusters-features.csv** - Contains the extracted features of a track using its timbres' collective cluster probabilities\n",
    "\n",
    "### Folder: million-song-subset\n",
    "\n",
    "- **tracks-features.csv** - Contains mfcc features extracted from tracks in the MSS\n",
    "- **tracks-timbres.csv** - Contains segment timbres for all tracks\n",
    "\n",
    "### Folder: taste-profile-subset\n",
    "\n",
    "- **songs.txt** - A list of song ids\n",
    "- **users.txt** - A list of user ids\n",
    "- **train-triplets.txt** - A user-song-count triplets\n",
    "- **song-to-tracks.txt** - A song-track id mapping\n",
    "\n",
    "# Collaborative Filtering\n",
    "\n",
    "## Finding Optimal Number of Track Clusters (Based on Tracks' MFCC Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = root_path + \"data/\"\n",
    "\n",
    "n_jobs = -1\n",
    "max_iter = 500\n",
    "algorithm = \"full\"\n",
    "n_init = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_data = pd.read_csv(local_path + \"features/tracks-mfcc.csv\", sep=\"\\t\")\n",
    "\n",
    "cols = tracks_data.columns.tolist()[1:]\n",
    "tracks_features = tracks_data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    (n_clusters, KMeans(n_clusters=n_clusters, random_state=0, n_jobs=n_jobs, max_iter=max_iter, algorithm=algorithm, n_init=n_init))\n",
    "    for n_clusters in range(5, 16, 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_clusters, estimator in estimators:\n",
    "    estimator.fit(tracks_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_path + \"features/tracks-clustering-kmeans-inertias.csv\", \"w\") as f:\n",
    "    cluster_inertias = []\n",
    "    \n",
    "    for n_clusters, estimator in estimators:\n",
    "        cluster_inertias.append([n_clusters, estimator.inertia_])\n",
    "        \n",
    "    f.write(\"\\n\".join([str(n_clusters) + \"\\t\" + str(inertia) for n_clusters, inertia in cluster_inertias]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inertial Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_path + \"features/tracks-clustering-kmeans-inertias.csv\") as f:\n",
    "    cluster_inertias = [line.strip(\" \\t\\n\\r\").split(\"\\t\") for line in f.readlines()]\n",
    "    \n",
    "cluster_inertias = [[int(cluster), float(inertia)] for cluster, inertia in cluster_inertias]\n",
    "cluster_inertias = np.array(cluster_inertias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(cluster_inertias[:, 0], cluster_inertias[:, 1])\n",
    "\n",
    "plt.title(\"Tracks Clustering: Inertia for K-Means\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Variance\")\n",
    "\n",
    "plt.savefig(local_path + \"plots/tracks-clustering-kmeans-inertia.png\", dpi=250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Plot of Tracks MFCC (for 10 Clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_tracks_features = PCA(n_components=2).fit(tracks_features).transform(tracks_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters, estimator = estimators[5]\n",
    "cluster_assignments = estimator.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(decomposed_tracks_features[:, 0], decomposed_tracks_features[:, 1], alpha=.8, s=0.7)\n",
    "    \n",
    "plt.title(\"Tracks MFCC: PCA Plot\")\n",
    "    \n",
    "plt.savefig(local_path + \"plots/tracks-mfcc-pca.png\", dpi=250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Tracks using GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = root_path + \"data/\"\n",
    "\n",
    "n_clusters = 10\n",
    "max_iter = 5000\n",
    "covariance_type = \"diag\"\n",
    "n_init = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_data = pd.read_csv(local_path + \"features/tracks-mfcc.csv\", sep=\"\\t\")\n",
    "\n",
    "cols = tracks_data.columns[1:]\n",
    "tracks_mfcc = tracks_data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = GaussianMixture(n_components=n_clusters, covariance_type=covariance_type, max_iter=max_iter, random_state=0, n_init=n_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(tracks_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(estimator, local_path + \"models/tracks-clustering-gmm-model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = joblib.load(local_path + \"models/tracks-clustering-gmm-model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = estimator.predict_proba(tracks_mfcc)\n",
    "cluster_assignments = estimator.predict(tracks_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_path + \"tracks-cluster-probabilities.csv\", \"w\") as f:\n",
    "    for i, song_id in enumerate(tracks_data[\"id\"]):\n",
    "        params = [song_id] + list(probs[i]) + [cluster_assignments[i]]\n",
    "\n",
    "        params = [str(param) for param in params]\n",
    "\n",
    "        f.write(\"\\t\".join(params) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Plot of Tracks MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_tracks_mfcc = LinearDiscriminantAnalysis(n_components=2).fit(tracks_mfcc, cluster_assignments).transform(tracks_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    plt.scatter(decomposed_tracks_mfcc[cluster_assignments == i, 0], decomposed_tracks_mfcc[cluster_assignments == i, 1], alpha=.8, s=0.7)\n",
    "    \n",
    "plt.gca().set_xlim([-16, 6])\n",
    "plt.gca().set_ylim([-5, 5])\n",
    "plt.title(\"Tracks MFCC: LDA Plot (After GMM)\")\n",
    "    \n",
    "plt.savefig(local_path + \"plots/tracks-mfcc-gmm-clustering-pca.png\", dpi=250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Users to Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = root_path + \"data/taste-profile-subset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_to_tracks = dict()\n",
    "count = 0\n",
    "with open(local_path + \"songs-to-tracks.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip(\" \\t\\n\\r\").split()\n",
    "        if len(line) > 1:\n",
    "            songs_to_tracks[line[0]] = line[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(local_path + \"user-track-counts-raw.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_path + \"user-song-counts.txt\", \"r\") as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        line = line.strip(\" \\t\\n\\r\").split()\n",
    "        if len(line) == 3 and line[1] in songs_to_tracks:\n",
    "            for track in songs_to_tracks[line[1]]:\n",
    "                outfile.write(\"\\t\".join([line[0], track, line[2]]) + \"\\n\")\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Users into Training and Evaluation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = root_path + \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "%%bash -s \"$local_path\"\n",
    "\n",
    "cd $1/taste-profile-subset\n",
    "\n",
    "cut -f1 user-track-counts-raw.txt | sort | uniq -c > user-counts.txt\n",
    "cat user-counts.txt | sed 's/^ *\\([0-9]*\\) /\\1\\t/g' | awk '($1 > 49)' > t; mv t user-counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "%%bash -s \"$local_path\"\n",
    "\n",
    "cd $1/taste-profile-subset/\n",
    "\n",
    "awk 'BEGIN {\n",
    "    FS = OFS = \"\\t\"\n",
    "}\n",
    "NR == FNR {\n",
    "    f[$2] = $0\n",
    "    next\n",
    "}\n",
    "$1 in f {\n",
    "    print $0\n",
    "}' user-counts.txt user-track-counts-raw.txt > t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "%%bash -s \"$local_path\"\n",
    "\n",
    "cd $1\n",
    "\n",
    "awk 'BEGIN {\n",
    "    FS = OFS = \"\\t\"\n",
    "}\n",
    "NR == FNR {\n",
    "    f[$1] = 1\n",
    "    next\n",
    "}\n",
    "$2 in f {\n",
    "    print $0\n",
    "}' features/tracks-cluster-probabilities.csv taste-profile-subset/t > taste-profile-subset/user-track-counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "%%bash -s \"$local_path\"\n",
    "\n",
    "cd $1/taste-profile-subset/\n",
    "\n",
    "cut -f2 -d$'\\t' user-counts.txt | sort --random-sort > t\n",
    "\n",
    "size=`cat user-counts.txt | wc -l`\n",
    "vsize=$(( $size / 10 ))\n",
    "\n",
    "head -$vsize t > users-validation.txt\n",
    "tail -n+$vsize t > users-train.txt\n",
    "\n",
    "rm t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "%%bash -s \"$local_path\"\n",
    "\n",
    "cd $1/taste-profile-subset/\n",
    "\n",
    "awk 'BEGIN {\n",
    "    FS = OFS = \"\\t\"\n",
    "}\n",
    "NR == FNR {\n",
    "    f[$1] = 1\n",
    "    next\n",
    "}\n",
    "$1 in f {\n",
    "    print $0\n",
    "}' users-train.txt user-track-counts.txt > user-track-counts-train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "%%bash -s \"$local_path\"\n",
    "\n",
    "cd $1/taste-profile-subset/\n",
    "\n",
    "awk 'BEGIN {\n",
    "    FS = OFS = \"\\t\"\n",
    "}\n",
    "NR == FNR {\n",
    "    f[$1] = 1\n",
    "    next\n",
    "}\n",
    "$1 in f {\n",
    "    print $0\n",
    "}' users-validation.txt user-track-counts.txt > user-track-counts-validation.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing User Features (Based on Tracks' Cluster Probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = root_path + \"data/\"\n",
    "\n",
    "n_clusters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_mfcc = dict()\n",
    "with open(local_path + \"features/tracks-cluster-probabilities.csv\", \"r\") as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        line = f.readline()\n",
    "        line = line.strip(\" \\t\\n\\r\").split()\n",
    "        if len(line) == 12:\n",
    "            tracks_mfcc[line[0]] = np.array([float(field) for field in line[1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_path + \"taste-profile-subset/users-train.txt\") as f:\n",
    "    users_train = [user.strip(\" \\n\\r\") for user in f.readlines()]\n",
    "    \n",
    "with open(local_path + \"taste-profile-subset/users-validation.txt\") as f:\n",
    "    users_validation = [user.strip(\" \\n\\r\") for user in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = dict()\n",
    "user_track_counts = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_path + \"taste-profile-subset/user-track-counts.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\t\\n\\r\").split()\n",
    "        if len(line) == 3:\n",
    "            if line[0] not in user_track_counts:\n",
    "                user_features[line[0]] = np.zeros(n_clusters)\n",
    "                user_track_counts[line[0]] = 0\n",
    "                \n",
    "            user_features[line[0]] += tracks_mfcc[line[1]]\n",
    "            user_track_counts[line[0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_train = local_path + \"features/user-features-train.csv\"\n",
    "outfile_validation = local_path + \"features/user-features-validation.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outfile_train, \"w\") as f:\n",
    "    for user in users_train:\n",
    "        f.write(\"\\t\".join([user] + [str(field) for field in (user_features[user] / float(user_track_counts[user]))]) + \"\\n\")\n",
    "        \n",
    "with open(outfile_validation, \"w\") as f:\n",
    "    for user in users_validation:\n",
    "        f.write(\"\\t\".join([user] + [str(field) for field in (user_features[user] / float(user_track_counts[user]))]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Optimal Number of Users Clusters (Based on Users' Computed Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = root_path + \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = -1\n",
    "max_iter = 500\n",
    "algorithm = \"full\"\n",
    "n_init = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_csv(local_path + \"features/user-features-train.csv\", sep=\"\\t\", header=None)\n",
    "\n",
    "cols = user_data.columns.tolist()[1:]\n",
    "user_features = user_data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    (n_clusters, KMeans(n_clusters=n_clusters, random_state=0, n_jobs=n_jobs, max_iter=max_iter, algorithm=algorithm, n_init=n_init))\n",
    "    for n_clusters in range(10, 30, 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_clusters, estimator in estimators:\n",
    "    estimator.fit(user_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_path + \"features/users-clustering-kmeans-inertias.csv\", \"w\") as f:\n",
    "    cluster_inertias = []\n",
    "    \n",
    "    for n_clusters, estimator in estimators:\n",
    "        cluster_inertias.append([n_clusters, estimator.inertia_])\n",
    "        \n",
    "    f.write(\"\\n\".join([str(n_clusters) + \"\\t\" + str(inertia) for n_cluster, inertia in cluster_inertias]))\n",
    "    \n",
    "    cluster_inertias = np.array(cluster_inertias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inertial Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_path + \"features/users-clustering-kmeans-inertias.csv\") as f:\n",
    "    cluster_inertias = [line.strip(\" \\t\\n\\r\").split(\"\\t\") for line in f.readlines()]\n",
    "    \n",
    "cluster_inertias = [[int(cluster), float(inertia)] for cluster, inertia in cluster_inertias]\n",
    "cluster_inertias = np.array(cluster_inertias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(cluster_inertias[:, 0].astype(int), cluster_inertias[:, 1])\n",
    "\n",
    "plt.title(\"Users Clustering: Inertia for K-Means\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Variance\")\n",
    "\n",
    "plt.savefig(local_path + \"plots/users-clustering-kmeans-inertia.png\", dpi=250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Plot of Users MFCC (for 20 Clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_user_features = PCA(n_components=2).fit(user_features).transform(user_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(decomposed_user_features[:, 0], decomposed_user_features[:, 1], alpha=.8, s=0.7)\n",
    "    \n",
    "plt.title(\"User Features: PCA Plot\")\n",
    "    \n",
    "plt.savefig(local_path + \"plots/user-features-pca.png\", dpi=250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Users using GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = root_path + \"data/\"\n",
    "\n",
    "n_clusters = 20\n",
    "max_iter = 5000\n",
    "covariance_type = \"diag\"\n",
    "n_init = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_csv(local_path + \"/features/user-features-train.csv\", sep=\"\\t\", header=None)\n",
    "\n",
    "cols = user_data.columns[1:]\n",
    "user_features = user_data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = GaussianMixture(n_components=n_clusters, covariance_type=covariance_type, max_iter=max_iter, random_state=0, n_init=n_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(user_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(estimator, local_path + \"/models/users-clustering-gmm-model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = joblib.load(local_path + \"/models/users-clustering-gmm-model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = estimator.predict_proba(user_features)\n",
    "cluster_assignments = estimator.predict(user_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(n_clusters):\n",
    "    with open(local_path + \"taste-profile-subset/clusters/user-ids-\" + str(cluster + 1) + \".txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(user_data[cluster_assignments == cluster][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_path + \"/features/user-cluster-probabilities.csv\", \"w\") as f:\n",
    "    for i, user_id in enumerate(user_data[user_data.columns[0]]):\n",
    "        params = [user_id] + list(probs[i]) + [cluster_assignments[i]]\n",
    "\n",
    "        params = [str(param) for param in params]\n",
    "\n",
    "        f.write(\"\\t\".join(params) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Plot of User Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_user_features = LinearDiscriminantAnalysis(n_components=2).fit(user_features, cluster_assignments).transform(user_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    plt.scatter(decomposed_user_features[cluster_assignments == i, 0], decomposed_user_features[cluster_assignments == i, 1], alpha=.8, rasterized=True, s=0.7)\n",
    "\n",
    "plt.gca().set_ylim([-15, 5])\n",
    "plt.title(\"User Features: LDA Plot (After GMM)\")\n",
    "    \n",
    "plt.savefig(local_path + \"plots/user-features-gmm-clustering-pca.png\", dpi=250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributing Users by their clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = root_path + \"data/taste-profile-subset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "%%bash -s \"$local_path\"\n",
    "\n",
    "cd $1/clusters/\n",
    "for cluster in {1..20}; do\n",
    "    cat user-ids-$cluster.txt | sed \"s/$/\\t$cluster/g\"\n",
    "    echo \"\"\n",
    "done > user-clusters.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_files = [open(local_path + \"clusters/user-track-counts-\" + str(cluster + 1) + \".txt\", \"w\") for cluster in range(n_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_clusters = dict()\n",
    "with open(local_path + \"clusters/user-clusters.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\"\\t\\n\\r\").split(\"\\t\")\n",
    "        user_clusters[line[0]] = int(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_path + \"user-track-counts.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\"\\t\\n\\r\").split(\"\\t\")\n",
    "        if line[0] in user_clusters:\n",
    "            cluster_files[user_clusters[line[0]] - 1].write(\"\\t\".join(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in cluster_files:\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering On User Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = root_path + \"data/taste-profile-subset/\"\n",
    "\n",
    "n_clusters = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_suggestions_file = open(local_path + \"suggestions.csv\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_track_counts = dict()\n",
    "\n",
    "with open(local_path + \"clustered-user-track-counts/cluster-k0.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\t\\n\\r\").split(\"\\t\")\n",
    "        if line != []:\n",
    "            user_track_counts[line[0]] = set(line[1:])\n",
    "    \n",
    "similarity = [[0]*len(user_track_counts)]*len(user_track_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = set([])\n",
    "for user in user_track_counts:\n",
    "    for track in user_track_counts[user]:\n",
    "        tracks.add(track)\n",
    "        \n",
    "tracks = list(tracks)\n",
    "users = list(user_track_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M = (len(users), len(tracks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, user_i in enumerate(users):\n",
    "    weights = dict()\n",
    "    for track in tracks:\n",
    "        weights[track] = 0\n",
    "        \n",
    "    for user_j in user_track_counts:\n",
    "        if user_i != user_j:\n",
    "        \n",
    "            similarity = len(user_track_counts[user_i].intersection(user_track_counts[user_j]))\n",
    "            similarity = similarity / (sqrt(len(user_track_counts[user_i])) * sqrt(len(user_track_counts[user_j])))\n",
    "\n",
    "            for track in user_track_counts[user_j]:\n",
    "                if track not in user_track_counts[user_i]:\n",
    "                    weights[track] += similarity\n",
    "                    \n",
    "    keys = sorted(list(weights), key=lambda x: -weights[x])[:50]\n",
    "    user_suggestions_file.write(user_i + \"\\t\" + \"\\t\".join(keys) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_suggestions_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Recommendations for Validation Users (User-User Localized Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import sqrt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = root_path + \"data/\"\n",
    "\n",
    "n_clusters = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = dict()\n",
    "with open(local_path + \"features/user-features-validation.csv\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\t\\n\\r\").split()\n",
    "        user_features[line[0]] = line[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(user_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_clustering_model = joblib.load(local_path + \"models/users-clustering-gmm-model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_users = dict()\n",
    "for cluster in range(n_clusters):\n",
    "    clustered_users[cluster] = []\n",
    "    \n",
    "for user in users:\n",
    "    cluster = gmm_clustering_model.predict([user_features[user]])[0]\n",
    "    clustered_users[cluster].append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tracks = dict()\n",
    "user_validation_tracks = dict()\n",
    "for user in users:\n",
    "    user_tracks[user] = [set([]), 0]\n",
    "    user_validation_tracks[user] = set([])\n",
    "\n",
    "with open(local_path + \"taste-profile-subset/user-track-counts-validation.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\n\\r\").split(\"\\t\")\n",
    "        if random.random() > 0.35:\n",
    "            user_tracks[line[0]][0].add(line[1])\n",
    "        else:\n",
    "            user_validation_tracks[line[0]].add(line[1])\n",
    "\n",
    "for user in users:\n",
    "    user_tracks[user][1] = sqrt(len(user_tracks[user][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions_for_cluster(cluster):\n",
    "    global user_tracks, clustered_users, local_path\n",
    "    \n",
    "    outfile = open(local_path + \"taste-profile-subset/suggestions-validation-\" + str(cluster) + \".txt\", \"w\")\n",
    "    \n",
    "    print \"Starting for cluster\", cluster\n",
    "    tracks = set([])\n",
    "    \n",
    "    cluster_user_tracks = dict()\n",
    "    with open(local_path + \"taste-profile-subset/clusters/user-ids-\" + str(cluster + 1) + \".txt\") as f:\n",
    "        for line in f:\n",
    "            cluster_user_tracks[line.strip(\" \\n\\r\")] = [set([]), 0]\n",
    "\n",
    "    with open(local_path + \"taste-profile-subset/clusters/user-track-counts-\" + str(cluster + 1) + \".txt\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip(\" \\n\\r\").split(\"\\t\")\n",
    "            cluster_user_tracks[line[0]][0].add(line[1])\n",
    "            tracks.add(line[1])\n",
    "\n",
    "    for user in cluster_user_tracks:\n",
    "        cluster_user_tracks[user][1] = sqrt(len(cluster_user_tracks[user][0]))\n",
    "\n",
    "    for i, user_v in enumerate(clustered_users[cluster]):\n",
    "        if i % 10 == 0:\n",
    "            print \"\\tStarting for user\", i\n",
    "            \n",
    "        track_weights = dict()\n",
    "        for track in tracks:\n",
    "            track_weights[track] = 0\n",
    "            \n",
    "        for user_t in cluster_user_tracks:\n",
    "            similarity = len(user_tracks[user_v][0].intersection(cluster_user_tracks[user_t][0]))\n",
    "            similarity = similarity / (user_tracks[user_v][1] * cluster_user_tracks[user_t][1])\n",
    "            similarity = pow(similarity, 6)\n",
    "\n",
    "            for track in cluster_user_tracks[user_t][0].difference(user_tracks[user_v][0]):\n",
    "                track_weights[track] += similarity\n",
    "        \n",
    "        suggestions = np.array(sorted(tracks, key=lambda x: track_weights[x]))[-500:]\n",
    "        suggestions = set(suggestions[np.searchsorted([track_weights[track] for track in suggestions], 0, side=\"right\"):])\n",
    "        \n",
    "        outfile.write(user_v + \"\\t\" + \"\\t\".join(suggestions) + \"\\n\")\n",
    "        \n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_pool = Pool(4)\n",
    "for i in range(n_clusters):\n",
    "    process_pool.map(get_suggestions_for_cluster, range(n_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "%%bash -s \"$local_path\"\n",
    "\n",
    "cd $1/taste-profile-subset\n",
    "\n",
    "for cluster in {1..20}; do\n",
    "    cat suggestions-validation-$cluster.txt\n",
    "done > suggestions-validation.txt\n",
    "\n",
    "for cluster in {1..20}; do\n",
    "    rm suggestions-validation-$cluster.txt\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_path + \"taste-profile-subset/user-tracks-used-validation.txt\", \"w\") as f:\n",
    "    for user in user_tracks:\n",
    "        f.write(user + \"\\t\" + \"\\t\".join(user_tracks[user][0]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Recommendations for Validation Users (Item-Item Localized Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import sqrt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = root_path + \"data/\"\n",
    "\n",
    "n_clusters = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = dict()\n",
    "with open(local_path + \"features/user-features-validation.csv\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\t\\n\\r\").split()\n",
    "        user_features[line[0]] = line[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(user_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_clustering_model = joblib.load(local_path + \"models/users-clustering-gmm-model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tracks = dict()\n",
    "user_validation_tracks = dict()\n",
    "for user in users:\n",
    "    user_tracks[user] = [set([]), 0]\n",
    "    user_validation_tracks[user] = set([])\n",
    "    \n",
    "track_users = dict()\n",
    "\n",
    "with open(local_path + \"taste-profile-subset/user-track-counts-validation.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\n\\r\").split(\"\\t\")\n",
    "        if random.random() > 0.35:\n",
    "            if line[1] not in track_users:\n",
    "                track_users[line[1]] = [set([]), 0]\n",
    "                \n",
    "            track_users[line[1]][0].add(line[0])\n",
    "            user_tracks[line[0]][0].add(line[1])\n",
    "        else:\n",
    "            user_validation_tracks[line[0]].add(line[1])\n",
    "\n",
    "for user in users:\n",
    "    user_tracks[user][1] = sqrt(len(user_tracks[user][0]))\n",
    "\n",
    "for track in track_users:\n",
    "    track_users[track][1] = sqrt(len(track_users[track][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_users = dict()\n",
    "clustered_tracks = dict()\n",
    "for cluster in range(n_clusters):\n",
    "    clustered_users[cluster] = []\n",
    "    clustered_tracks[cluster] = set([])\n",
    "    \n",
    "for user in users:\n",
    "    cluster = gmm_clustering_model.predict([user_features[user]])[0]\n",
    "    \n",
    "    clustered_users[cluster].append(user)\n",
    "    clustered_tracks[cluster] = clustered_tracks[cluster].union(user_tracks[user][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions_for_cluster(cluster):\n",
    "    global track_users, clustered_users, clustered_tracks, local_path\n",
    "\n",
    "    outfile = open(local_path + \"taste-profile-subset/suggestions-validation-\" + str(cluster) + \".txt\", \"w\")\n",
    "\n",
    "    print \"Starting for cluster\", cluster\n",
    "    \n",
    "    cluster_track_users = dict()\n",
    "    with open(local_path + \"taste-profile-subset/clusters/user-track-counts-\" + str(cluster + 1) + \".txt\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip(\" \\n\\r\").split(\"\\t\")\n",
    "            if line[1] not in cluster_track_users:\n",
    "                cluster_track_users[line[1]] = [set([]), 0]\n",
    "                \n",
    "            cluster_track_users[line[1]][0].add(line[0])\n",
    "\n",
    "    for track in cluster_track_users:\n",
    "        cluster_track_users[track][1] = sqrt(len(cluster_track_users[track][0]))\n",
    "\n",
    "    for i, user_v in enumerate(list(clustered_users[cluster])):\n",
    "        if i % 10 == 9:\n",
    "            print \"\\tStarting for user\", i\n",
    "            outfile.close()\n",
    "            \n",
    "        suggestions = []\n",
    "            \n",
    "        for j, track_t in enumerate(list(cluster_track_users)):\n",
    "            similarity = 0\n",
    "            \n",
    "            for track_v in list(user_tracks[user_v][0]):\n",
    "                similarity_t = len(track_users[track_v][0].intersection(cluster_track_users[track_t][0]))\n",
    "                similarity_t = similarity / (track_users[track_v][1] * cluster_track_users[track_t][1])\n",
    "                similarity_t = pow(similarity, 3)\n",
    "                \n",
    "                similarity += similarity_t\n",
    "                \n",
    "            suggestions.append((track_t, similarity))\n",
    "                \n",
    "        suggestions.sort(key=lambda x: -x[1])\n",
    "        suggestions = suggestions[:500]\n",
    "        suggestions = suggestions[:np.searchsorted([track[1] for track in suggestions], 0, side=\"left\")]\n",
    "        \n",
    "        print suggestions\n",
    "        \n",
    "        outfile.write(user_v + \"\\t\" + \"\\t\".join(suggestions) + \"\\n\")\n",
    "        \n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_pool = Pool(1)\n",
    "# process_pool.map(get_suggestions_for_cluster, range(n_clusters))\n",
    "get_suggestions_for_cluster(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "%%bash -s \"$local_path\"\n",
    "\n",
    "cd $1/taste-profile-subset\n",
    "\n",
    "for cluster in {1..20}; do\n",
    "    cat suggestions-validation-$cluster.txt\n",
    "done > suggestions-validation.txt\n",
    "\n",
    "for cluster in {1..20}; do\n",
    "    rm suggestions-validation-$cluster.txt\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_path + \"taste-profile-subset/user-tracks-used-validation.txt\", \"w\") as f:\n",
    "    for user in user_tracks:\n",
    "        f.write(user + \"\\t\" + \"\\t\".join(user_tracks[user][0]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Truncated mAP on the Predicted Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = root_path + \"data/taste-profile-subset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listened_user_tracks = dict()\n",
    "with open(local_path + \"users-validation.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\n\\r\")\n",
    "        listened_user_tracks[line] = set([])\n",
    "        \n",
    "with open(local_path + \"user-track-counts-validation.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\n\\r\").split(\"\\t\")\n",
    "        listened_user_tracks[line[0]].add(line[1])\n",
    "\n",
    "for user in listened_user_tracks:\n",
    "    listened_user_tracks[user] = set(listened_user_tracks[user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_path + \"user-tracks-used-validation-uu2.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\n\\r\").split(\"\\t\")\n",
    "        listened_user_tracks[line[0]] = listened_user_tracks[line[0]].difference(line[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_path + \"suggestions-uu2.txt\") as f:\n",
    "    aps = list()\n",
    "    \n",
    "    for line in f:\n",
    "        if line.strip() == \"\":\n",
    "            continue\n",
    "            \n",
    "        line = line.strip(\" \\t\\n\\r\").split(\"\\t\")\n",
    "        \n",
    "        user = line[0]\n",
    "        tracks = line[1:]\n",
    "        tracks = np.array(tracks[:500])\n",
    "        \n",
    "        k = 0\n",
    "        l = 0\n",
    "        p = 0.0\n",
    "        for i, track in enumerate(tracks):\n",
    "            k += 1\n",
    "            if track in listened_user_tracks[user]:\n",
    "                l += 1\n",
    "                p += float(l) / float(k)\n",
    "            \n",
    "        if l != 0:\n",
    "            aps.append(p / l)\n",
    "        else:\n",
    "            aps.append(0)\n",
    "        \n",
    "    print np.mean(aps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.275001780514\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Plot of the User Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = root_path + \"data/\"\n",
    "\n",
    "n_clusters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listened_user_tracks = dict()\n",
    "with open(local_path + \"taste-profile-subset/users-validation.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\n\\r\")\n",
    "        listened_user_tracks[line] = set([])\n",
    "        \n",
    "with open(local_path + \"taste-profile-subset/user-track-counts-validation.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\n\\r\").split(\"\\t\")\n",
    "        listened_user_tracks[line[0]].add(line[1])\n",
    "\n",
    "for user in listened_user_tracks:\n",
    "    listened_user_tracks[user] = set(listened_user_tracks[user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_path + \"taste-profile-subset/user-tracks-used-validation-uu2.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\n\\r\").split(\"\\t\")\n",
    "        listened_user_tracks[line[0]] = listened_user_tracks[line[0]].difference(line[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_suggestions = dict()\n",
    "with open(local_path + \"taste-profile-subset/suggestions-uu2.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\t\\n\\r\").split(\"\\t\")\n",
    "        user_suggestions[line[0]] = set(line[1:]).difference(listened_user_tracks[user])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_clustering_model = joblib.load(local_path + \"models/tracks-clustering-gmm-model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_mfcc = []\n",
    "with open(local_path + \"features/tracks-mfcc.csv\") as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\t\\n\\r\").split()\n",
    "        tracks_mfcc.append([float(field) for field in line[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignments = tracks_clustering_model.predict(tracks_mfcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading User Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = list(user_suggestions)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tracks = listened_user_tracks[user]\n",
    "user_suggestions = user_suggestions[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tracks_mfcc = []\n",
    "user_suggestions_mfcc = []\n",
    "with open(local_path + \"features/tracks-mfcc.csv\") as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\t\\n\\r\").split()\n",
    "        if line[0] in user_tracks:\n",
    "            user_tracks_mfcc.append([float(field) for field in line[1:]])\n",
    "            \n",
    "        if line[0] in user_suggestions:\n",
    "            user_suggestions_mfcc.append([float(field) for field in line[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Plot of User Tracks and Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LinearDiscriminantAnalysis(n_components=2).fit(tracks_mfcc, cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_tracks_mfcc = lda_model.transform(tracks_mfcc)\n",
    "decomposed_user_tracks_mfcc = lda_model.transform(user_tracks_mfcc)\n",
    "decomposed_user_suggestions_mfcc = lda_model.transform(user_suggestions_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    plt.scatter(decomposed_tracks_mfcc[cluster_assignments == i, 0], decomposed_tracks_mfcc[cluster_assignments == i, 1], alpha=.8, rasterized=True, s=0.7)\n",
    "\n",
    "plt.scatter(decomposed_user_tracks_mfcc[:, 0], decomposed_user_tracks_mfcc[:, 1], alpha=1, s=8, c=\"blue\")\n",
    "plt.scatter(decomposed_user_suggestions_mfcc[:, 0], decomposed_user_suggestions_mfcc[:, 1], alpha=1, s=8, c=\"black\")\n",
    "\n",
    "plt.gca().set_xlim([-15, 5])\n",
    "plt.gca().set_ylim([-4, 4.5])\n",
    "plt.title(\"Tracks MFCC: LDA Plot (After GMM)\")\n",
    "\n",
    "plt.savefig(local_path + \"plots/tracks-mfcc-lda-exploited-suggestions.png\", dpi=250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "\n",
    "## Generating Track Recommendations through Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = root_path + \"data/\"\n",
    "\n",
    "n_clusters = 10\n",
    "n_suggestions = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = set()\n",
    "tracks = set()\n",
    "with open(local_path + \"taste-profile-subset/user-track-counts.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\t\\n\\r\").split(\"\\t\")\n",
    "        \n",
    "        users.add(line[0])\n",
    "        tracks.add(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(users)\n",
    "tracks = list(tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_indices = dict()\n",
    "track_indices = dict()\n",
    "\n",
    "for i, user in enumerate(users):\n",
    "    user_indices[user] = i\n",
    "\n",
    "for i, track in enumerate(tracks):\n",
    "    track_indices[track] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tracks = dict()\n",
    "track_features = dict()\n",
    "\n",
    "for i in range(len(users)):\n",
    "    user_tracks[i] = set()\n",
    "\n",
    "for i in range(len(tracks)):\n",
    "    track_features[i] = [0, -1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(local_path + \"taste-profile-subset/user-track-counts.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\t\\n\\r\").split(\"\\t\")\n",
    "        \n",
    "        user, track = user_indices[line[0]], track_indices[line[1]]\n",
    "        user_tracks[user].add(track)\n",
    "        track_features[track][0] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_tracks = dict()\n",
    "for cluster in range(n_clusters):\n",
    "    clustered_tracks[cluster] = []\n",
    "    \n",
    "with open(local_path + \"features/tracks-cluster-probabilities.csv\") as f:\n",
    "    for track in f:\n",
    "        track = track.strip(\" \\t\\n\\r\").split(\"\\t\")\n",
    "        if track[0] in track_indices:\n",
    "            track[0] = track_indices[track[0]]\n",
    "            \n",
    "            clustered_tracks[int(track[-1])].append(track[0])\n",
    "            track_features[track[0]][1] = int(track[-1])\n",
    "            track_features[track[0]][2] = float(track[int(track[-1]) + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_features[list(track_features)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in clustered_tracks:\n",
    "    clustered_tracks[cluster].sort(key=lambda track: -track_features[track][0] * track_features[track][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tracks_clusters = dict()\n",
    "for user in user_tracks:\n",
    "    user_tracks_clusters[user] = []\n",
    "    for cluster in range(n_clusters):\n",
    "        user_tracks_clusters[user].append(1)\n",
    "        \n",
    "    for track in user_tracks[user]:\n",
    "        user_tracks_clusters[user][track_features[track][1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in user_tracks:\n",
    "    normalization_const = 0\n",
    "    for cluster in range(n_clusters):\n",
    "        user_tracks_clusters[user][cluster] = sqrt(len(clustered_tracks[cluster])) / user_tracks_clusters[user][cluster]\n",
    "        normalization_const += user_tracks_clusters[user][cluster]\n",
    "    \n",
    "    for cluster in range(n_clusters):\n",
    "        user_tracks_clusters[user][cluster] = user_tracks_clusters[user][cluster] / normalization_const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(local_path + \"taste-profile-subset/suggestions-exploration.txt\", \"w\")\n",
    "\n",
    "user_suggestions = dict()\n",
    "for user in user_tracks:\n",
    "    suggestions = set([])\n",
    "    cluster_indices = [0] * n_suggestions\n",
    "    \n",
    "    while len(suggestions) < n_suggestions:\n",
    "        cluster = np.argmax(np.random.multinomial(20, user_tracks_clusters[user], size = 1))\n",
    "        \n",
    "        while clustered_tracks[cluster][cluster_indices[cluster]] in user_tracks[user]:\n",
    "            cluster_indices[cluster] += 1\n",
    "        \n",
    "        suggestions.add(clustered_tracks[cluster][cluster_indices[cluster]])\n",
    "        cluster_indices[cluster] += 1\n",
    "    \n",
    "    user_suggestions[user] = suggestions\n",
    "    outfile.write(users[user] + \"\\t\" + \"\\t\".join([tracks[track] for track in suggestions]) + \"\\n\")\n",
    "    \n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting User Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = root_path + \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_clustering_model = joblib.load(local_path + \"models/tracks-clustering-gmm-model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_mfcc = []\n",
    "with open(local_path + \"features/tracks-mfcc.csv\") as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\t\\n\\r\").split()\n",
    "        tracks_mfcc.append([float(field) for field in line[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignments = tracks_clustering_model.predict(tracks_mfcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Tracks for First User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_suggestions = []\n",
    "with open(local_path + \"taste-profile-subset/suggestions-exploration.txt\") as f:\n",
    "    user = f.readline().split(\"\\t\")\n",
    "    user_suggestions = user[1:]\n",
    "    user = user[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tracks = []\n",
    "with open(local_path + \"taste-profile-subset/user-track-counts.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\t\\n\\r\").split(\"\\t\")\n",
    "        \n",
    "        if line[0] == user:\n",
    "            user_tracks.append(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tracks_mfcc = []\n",
    "user_suggestions_mfcc = []\n",
    "with open(local_path + \"features/tracks-mfcc.csv\") as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        line = line.strip(\" \\t\\n\\r\").split()\n",
    "        if line[0] in user_tracks:\n",
    "            user_tracks_mfcc.append([float(field) for field in line[1:]])\n",
    "            \n",
    "        if line[0] in user_suggestions:\n",
    "            user_suggestions_mfcc.append([float(field) for field in line[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Plot of Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LinearDiscriminantAnalysis(n_components=2).fit(tracks_mfcc, cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_tracks_mfcc = lda_model.transform(tracks_mfcc)\n",
    "decomposed_user_tracks_mfcc = lda_model.transform(user_tracks_mfcc)\n",
    "decomposed_user_suggestions_mfcc = lda_model.transform(user_suggestions_mfcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for i in range(n_clusters):\n",
    "    plt.scatter(decomposed_tracks_mfcc[cluster_assignments == i, 0], decomposed_tracks_mfcc[cluster_assignments == i, 1], alpha=.8, rasterized=True, s=0.7)\n",
    "\n",
    "plt.scatter(decomposed_user_tracks_mfcc[:, 0], decomposed_user_tracks_mfcc[:, 1], alpha=1, s=8, c=\"blue\")\n",
    "plt.scatter(decomposed_user_suggestions_mfcc[:, 0], decomposed_user_suggestions_mfcc[:, 1], alpha=1, s=15, c=\"black\")\n",
    "\n",
    "plt.gca().set_xlim([-15, 5])\n",
    "plt.gca().set_ylim([-4, 4.5])\n",
    "plt.title(\"Tracks MFCC: LDA Plot (After GMM)\")\n",
    "\n",
    "plt.savefig(local_path + \"plots/tracks-mfcc-lda-explored-suggestions.png\", dpi=250)\n",
    "plt.show()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
